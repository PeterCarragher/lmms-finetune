{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/home/pcarragh/miniconda3/envs/lmms-finetune/lib/python3.10/site-packages/torch/lib/../../nvidia/cusparse/lib/libcusparse.so.12: undefined symbol: __nvJitLinkComplete_12_4, version libnvJitLink.so.12",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lmms-finetune/lib/python3.10/site-packages/torch/__init__.py:367\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[1;32m    366\u001b[0m         _load_global_deps()\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSymInt\u001b[39;00m:\n\u001b[1;32m    371\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m    Like an int (including magic methods), but redirects all operations on the\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    wrapped node. This is used in particular to symbolically record operations\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    in the symbolic shape workflow.\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: /home/pcarragh/miniconda3/envs/lmms-finetune/lib/python3.10/site-packages/torch/lib/../../nvidia/cusparse/lib/libcusparse.so.12: undefined symbol: __nvJitLinkComplete_12_4, version libnvJitLink.so.12"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"LD_LIBRARY_PATH\"] =\"/home/pcarragh/miniconda3/envs/lmms-finetune/lib/python3.10/site-packages/nvidia/nvjitlink/lib:/usr/local/cuda-12/lib64:/usr/local/cuda-12/lib64:\"\n",
    "# \"/usr/local/cuda/lib64:/usr/local/cuda/lib64:\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from eval.eval_utils import *\n",
    "\n",
    "eval_data = json.load(open(\"/home/pcarragh/dev/webqa/MultiModalQA/data/WebQA_train_val_obj_v2.json\", \"r\"))\n",
    "eval_data = {k: v for k, v in eval_data.items() if v['Qcate'].lower() == 'yesno'}\n",
    "perturbation_path = \"/home/pcarragh/dev/webqa/image_gen_val/val_images_perturbed_gpt_obj_lama\"\n",
    "use_split = False\n",
    "save = False\n",
    "keys = list(eval_data.keys())\n",
    "version = \"1\"\n",
    "\n",
    "model_path = \"Qwen/Qwen2-VL-7B-Instruct\" # \"Qwen/Qwen2-VL-72B-Instruct-AWQ\" #\n",
    "model, processor = get_model_processor(model_path)\n",
    "conversational_prompt = not 'Phi' in model_path\n",
    "\n",
    "results = {}\n",
    "output_file = \"qa_check_counterfactuals\"\n",
    "with open(f\"data/{output_file}_v{version}.csv\", \"w\") as f:\n",
    "    f.write(\"model,question_id,image_id,qa_check\\n\")\n",
    "\n",
    "qa_check_answers = {}\n",
    "for k in tqdm(keys): \n",
    "    example = eval_data[k]\n",
    "    qa_check_answers[k] = {}\n",
    "    for img in example['img_posFacts']:\n",
    "        original_image_file = str(img['image_id'])\n",
    "        counterfactual_image_file = f\"{perturbation_path}/{original_image_file}_{k}.jpeg\"\n",
    "        \n",
    "        question = f\"Q: was the {example['Q_obj']} from the original image removed in the perturbed image?\"\n",
    "        messages = get_qa_check_prompt(question, conversational_prompt)\n",
    "        try:\n",
    "            images = get_images([original_image_file, counterfactual_image_file])\n",
    "        except Exception as e:\n",
    "            # print(f\"Error: {e}\")\n",
    "            continue\n",
    "        qa_check = run_inference(messages, images, processor, model, conversational_prompt)    \n",
    "        qa_check_answers[k][original_image_file] = qa_check\n",
    "        with open(f\"data/{output_file}_v{version}.csv\", \"a\") as f:\n",
    "            f.write(f\"{model_path},{k},{original_image_file},{qa_check}\\n\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qa_check_answers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# compare perturbed and original image side by side\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m perturbation_detected_keys \u001b[38;5;241m=\u001b[39m [(k, img_id) \u001b[38;5;28;01mfor\u001b[39;00m k,imgs \u001b[38;5;129;01min\u001b[39;00m \u001b[43mqa_check_answers\u001b[49m\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mfor\u001b[39;00m img_id,ans \u001b[38;5;129;01min\u001b[39;00m imgs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ans\u001b[38;5;241m.\u001b[39mlower()]\n\u001b[1;32m      5\u001b[0m perturbation_undetected_keys \u001b[38;5;241m=\u001b[39m [(k, img_id) \u001b[38;5;28;01mfor\u001b[39;00m k,imgs \u001b[38;5;129;01min\u001b[39;00m qa_check_answers\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mfor\u001b[39;00m img_id,ans \u001b[38;5;129;01min\u001b[39;00m imgs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ans\u001b[38;5;241m.\u001b[39mlower()]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# example_keys = [(k, id) for k, id in perturbation_detected_keys if len(eval_data[k]['img_posFacts']) == 1]\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qa_check_answers' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# compare perturbed and original image side by side\n",
    "perturbation_detected_keys = [(k, img_id) for k,imgs in qa_check_answers.items() for img_id,ans in imgs.items() if 'yes' in ans.lower()]\n",
    "perturbation_undetected_keys = [(k, img_id) for k,imgs in qa_check_answers.items() for img_id,ans in imgs.items() if 'no' in ans.lower()]\n",
    "# example_keys = [(k, id) for k, id in perturbation_detected_keys if len(eval_data[k]['img_posFacts']) == 1]\n",
    "example_keys = perturbation_undetected_keys\n",
    "\n",
    "key, image_id = example_keys[random.randint(0, len(example_keys))]\n",
    "imgs = get_images([image_id, f\"{perturbation_path}/{str(image_id)}_{key}.jpeg\"])\n",
    "q = capitalize_word_in_sentence(eval_data[key]['Q'], eval_data[key]['Q_obj'])\n",
    "\n",
    "print(image_equals(imgs[0], imgs[1]))\n",
    "print(qa_check_answers[key][image_id])\n",
    "display_images(imgs[0], imgs[1], q, \"original\", \"perturbed\", savefile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# print(len(perturbation_detected_keys))\n",
    "# # dump keys to csv\n",
    "# import csv\n",
    "# with open(f\"results/counterfactual_qa_check.csv\", \"w\") as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerows(perturbation_detected_keys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmms-finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
